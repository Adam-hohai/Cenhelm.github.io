<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8" />

    

    

    <title>XAI with code | Cenehlm&#39;s blogs</title>
    <meta name="author" content="Cenehlm" />
    <meta name="keywords" content="" />
    <meta name="description" content="Explainable artificial intelligence with code 88篇论文汇总Axiomatic Attribution for Deep Networks2017 2557integrated gradients，将深度网络的预测归因于其输入。GNNExplainer: Generating Explanations for Graph Neural Networks2019 316为图神经网络生成解释，利用图NN的递归邻域聚合方案去辨别重要的图路径，突出显示沿着路" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no" />

    
    <link rel="alternate" href="/cenhelm.github.io/atom.xml" title="Cenehlm&#39;s blogs" type="application/atom+xml">
    
    
    <link rel="icon" href="/cenhelm.github.io/images/favicon.ico">
    

    <style type="text/css">
    @font-face {
        font-family: 'icomoon';
        src: url("/cenhelm.github.io/fonts/icomoon.eot?q628ml");
        src: url("/cenhelm.github.io/fonts/icomoon.eot?q628ml#iefix") format('embedded-opentype'),
             url("/cenhelm.github.io/fonts/icomoon.ttf?q628ml") format('truetype'),
             url("/cenhelm.github.io/fonts/icomoon.woff?q628ml") format('woff'),
             url("/cenhelm.github.io/fonts/icomoon.svg?q628ml#icomoon") format('svg');
        font-weight: normal;
        font-style: normal;
    }
    </style>
    
<link rel="stylesheet" href="/cenhelm.github.io/css/style.css">


    <!--[if lt IE 9]><style type="text/css">.nav-inner {top:0;}.author-meta {position:static;top:0;}.search-form {height:36px;}</style><script type="text/javascript" src="https://unpkg.com/html5shiv@3.7.3/dist/html5shiv.min.js"></script><![endif]-->
<meta name="generator" content="Hexo 5.4.0"></head>
<body>

    <main class="app">
        <header id="header" class="header clearfix">
    <div id="nav" class="nav">
    <div class="nav-mobile">
        <button id="open-panel" class="open-panel nav-mobile-item"><i class="icon-documents"></i></button>
        <h1 class="nav-mobile-title nav-mobile-item">Cenehlm&#39;s blogs</h1>
        <button id="open-menus" class="open-panel nav-mobile-item"><i class="icon-library"></i></button>
    </div>

    <nav id="nav-inner" class="nav-inner">
        
            <a class="nav-item" href="/cenhelm.github.io/">
                <span class="nav-text">首页</span>
            </a>
        
            <a class="nav-item" href="/cenhelm.github.io/categories/%E6%97%A5%E5%B8%B8">
                <span class="nav-text">日常</span>
            </a>
        
            <a class="nav-item" href="/cenhelm.github.io/categories/%E6%AF%94%E8%B5%9B">
                <span class="nav-text">比赛</span>
            </a>
        
            <a class="nav-item" href="/cenhelm.github.io/categories/%E7%A7%91%E7%A0%94">
                <span class="nav-text">科研</span>
            </a>
        
            <a class="nav-item" href="/cenhelm.github.io/archives">
                <span class="nav-text">归档</span>
            </a>
        
            <a class="nav-item" href="/cenhelm.github.io/about">
                <span class="nav-text">关于</span>
            </a>
        
    </nav>
</div>

    <aside id="aside" class="aside">
    <div id="aside-mask" class="aside-mask"></div>
    <div id="aside-inner" class="aside-inner">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit"><i class="icon-search-stroke"></i></button><input type="hidden" name="sitesearch" value="https://adam-hohai.github.io/cenhelm.github.io"></form>

        
        
        
        

        
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Axiomatic-Attribution-for-Deep-Networks"><span class="toc-number">1.</span> <span class="toc-text">Axiomatic Attribution for Deep Networks</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#GNNExplainer-Generating-Explanations-for-Graph-Neural-Networks"><span class="toc-number">2.</span> <span class="toc-text">GNNExplainer: Generating Explanations for Graph Neural Networks</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Proposed-Guidelines-for-the-Responsible-Use-of-Explainable-Machine-Learning"><span class="toc-number">3.</span> <span class="toc-text">Proposed Guidelines for the Responsible Use of Explainable Machine Learning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Software-for-Dataset-wide-XAI-From-Local-Explanations-to-Global-Insights-with-Zennit-CoRelAy-and-ViRelAy"><span class="toc-number">4.</span> <span class="toc-text">Software for Dataset-wide XAI: From Local Explanations to Global Insights with Zennit, CoRelAy, and ViRelAy</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SoPa-Bridging-CNNs-RNNs-and-Weighted-Finite-State-Machines"><span class="toc-number">5.</span> <span class="toc-text">SoPa: Bridging CNNs, RNNs, and Weighted Finite-State Machines</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DO-NOT-TRUST-ADDITIVE-EXPLANATIONS"><span class="toc-number">6.</span> <span class="toc-text">DO NOT TRUST ADDITIVE EXPLANATIONS</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#On-the-Explanation-of-Machine-Learning-Predictions-in-Clinical-Gait-Analysis"><span class="toc-number">7.</span> <span class="toc-text">On the Explanation of Machine Learning Predictions in Clinical Gait Analysis</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#EUCA-the-End-User-Centered-Explainable-AI-Framework"><span class="toc-number">8.</span> <span class="toc-text">EUCA: the End-User-Centered Explainable AI Framework</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Towards-Rigorous-Interpretations-a-Formalisation-of-Feature-Attribution"><span class="toc-number">9.</span> <span class="toc-text">Towards Rigorous Interpretations: a Formalisation of Feature Attribution</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Entropy-based-Logic-Explanations-of-Neural-Networks%E5%9F%BA%E4%BA%8E%E7%86%B5%E7%9A%84%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%80%BB%E8%BE%91%E8%A7%A3%E9%87%8A"><span class="toc-number">10.</span> <span class="toc-text">Entropy-based Logic Explanations of Neural Networks基于熵的神经网络逻辑解释</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Quantitative-Evaluation-of-Explainable-Graph-Neural-Networks-for-Molecular-Property-Prediction%E5%88%86%E5%AD%90%E6%80%A7%E8%B4%A8%E9%A2%84%E6%B5%8B%E4%B8%AD%E5%8F%AF%E8%A7%A3%E9%87%8A%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AE%9A%E9%87%8F%E8%AF%84%E4%BB%B7"><span class="toc-number">11.</span> <span class="toc-text">Quantitative Evaluation of Explainable Graph Neural Networks for Molecular Property Prediction分子性质预测中可解释图神经网络的定量评价</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Explaining-deep-learning-models-for-spoofing-and-deepfake-detection-with-SHapley-Additive-exPlanations"><span class="toc-number">12.</span> <span class="toc-text">Explaining deep learning models for spoofing and deepfake detection with SHapley Additive exPlanations</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#GAM-e-changer-or-not-An-evaluation-of-interpretable-machine-learning-models-based-on-additive-model-constraints"><span class="toc-number">13.</span> <span class="toc-text"># GAM(e) changer or not? An evaluation of interpretable machine learning models based on additive model constraints</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Visual-Interpretability-for-Deep-Learning-a-Survey"><span class="toc-number">14.</span> <span class="toc-text"># Visual Interpretability for Deep Learning: a Survey</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Meaningful-Data-Sampling-for-a-Faithful-Local-Explanation-Method"><span class="toc-number">15.</span> <span class="toc-text"># Meaningful Data Sampling for a Faithful Local Explanation Method</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Explainable-Artificial-Intelligence-XAI-Concepts-Taxonomies-Opportunities-and-Challenges-toward-Responsible-AI"><span class="toc-number">16.</span> <span class="toc-text"># Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Towards-Best-Practice-in-Explaining-Neural-Network-Decisions-with-LRP"><span class="toc-number">17.</span> <span class="toc-text"># Towards Best Practice in Explaining Neural Network Decisions with LRP</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#bLIMEy-Surrogate-Prediction-Explanations-Beyond-LIME"><span class="toc-number">18.</span> <span class="toc-text"># bLIMEy: Surrogate Prediction Explanations Beyond LIME</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Rule-Extraction-in-Unsupervised-Anomaly-Detection-for-Model-Explainability-Application-to-OneClass-SVM"><span class="toc-number">19.</span> <span class="toc-text"># Rule Extraction in Unsupervised Anomaly Detection for Model Explainability: Application to OneClass SVM</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#What-Would-You-Ask-the-Machine-Learning-Model-Identification-of-User-Needs-for-Model-Explanations-Based-on-Human-Model-Conversations"><span class="toc-number">20.</span> <span class="toc-text"># What Would You Ask the Machine Learning Model? Identification of User Needs for Model Explanations Based on Human-Model Conversations</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Learning-to-Structure-an-Image-with-Few-Colors"><span class="toc-number">21.</span> <span class="toc-text"># Learning to Structure an Image with Few Colors</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#The-Grammar-of-Interactive-Explanatory-Model-Analysis"><span class="toc-number">22.</span> <span class="toc-text"># The Grammar of Interactive Explanatory Model Analysis</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Applying-Genetic-Programming-to-Improve-Interpretability-in-Machine-Learning-Models"><span class="toc-number">23.</span> <span class="toc-text"># Applying Genetic Programming to Improve Interpretability in Machine Learning Models</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Explanations-of-Black-Box-Model-Predictions-by-Contextual-Importance-and-Utility"><span class="toc-number">24.</span> <span class="toc-text"># Explanations of Black-Box Model Predictions by Contextual Importance and Utility</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Explaining-Local-Global-And-Higher-Order-Interactions-In-Deep-Learning"><span class="toc-number">25.</span> <span class="toc-text"># Explaining Local, Global, And Higher-Order Interactions In Deep Learning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Embedded-Encoder-Decoder-in-Convolutional-Networks-Towards-Explainable-AI"><span class="toc-number">26.</span> <span class="toc-text"># Embedded Encoder-Decoder in Convolutional Networks Towards Explainable AI</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#EXPLAN-Explaining-Black-box-Classifiers-using-Adaptive-Neighborhood-Generation"><span class="toc-number">27.</span> <span class="toc-text"># EXPLAN: Explaining Black-box Classifiers using Adaptive Neighborhood Generation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#XNAP-Making-LSTM-based-Next-Activity-Predictions-Explainable-by-Using-LRP"><span class="toc-number">28.</span> <span class="toc-text"># XNAP: Making LSTM-based Next Activity Predictions Explainable by Using LRP</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Quantifying-Explainability-of-Saliency-Methods-in-Deep-Neural-Networks-with-a-Synthetic-Dataset"><span class="toc-number">29.</span> <span class="toc-text"># Quantifying Explainability of Saliency Methods in Deep Neural Networks with a Synthetic Dataset</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#TripleTree-A-Versatile-Interpretable-Representation-of-Black-Box-Agents-and-their-Environments"><span class="toc-number">30.</span> <span class="toc-text"># TripleTree: A Versatile Interpretable Representation of Black Box Agents and their Environments</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SCOUTER-Slot-Attention-based-Classifier-for-Explainable-Image-Recognition"><span class="toc-number">31.</span> <span class="toc-text"># SCOUTER: Slot Attention-based Classifier for Explainable Image Recognition</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Impact-of-lung-segmentation-on-the-diagnosis-and-explanation-of-COVID-19-in-chest-X-ray-images"><span class="toc-number">32.</span> <span class="toc-text"># Impact of lung segmentation on the diagnosis and explanation of COVID-19 in chest X-ray images</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Landscape-of-R-packages-for-eXplainable-Artificial-Intelligence"><span class="toc-number">33.</span> <span class="toc-text"># Landscape of R packages for eXplainable Artificial Intelligence</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Causal-Shapley-Values-Exploiting-Causal-Knowledge-to-Explain-Individual-Predictions-of-Complex-Models"><span class="toc-number">34.</span> <span class="toc-text"># Causal Shapley Values: Exploiting Causal Knowledge to Explain Individual Predictions of Complex Models</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Why-model-why-Assessing-the-strengths-and-limitations-of-LIME"><span class="toc-number">35.</span> <span class="toc-text"># Why model why? Assessing the strengths and limitations of LIME</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Driving-Behavior-Explanation-with-Multi-level-Fusion"><span class="toc-number">36.</span> <span class="toc-text"># Driving Behavior Explanation with Multi-level Fusion</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Checklist-for-responsible-deep-learning-modeling-of-medical-images-based-on-COVID-19-detection-studies"><span class="toc-number">37.</span> <span class="toc-text"># Checklist for responsible deep learning modeling of medical images based on COVID-19 detection studies</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#XAI-P-T-A-Brief-Review-of-Explainable-Artificial-Intelligence-from-Practice-to-Theory"><span class="toc-number">38.</span> <span class="toc-text"># XAI-P-T: A Brief Review of Explainable Artificial Intelligence from Practice to Theory</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#TorchPRISM-Principal-Image-Sections-Mapping-a-novel-method-for-Convolutional-Neural-Network-features-visualization"><span class="toc-number">39.</span> <span class="toc-text"># TorchPRISM: Principal Image Sections Mapping, a novel method for Convolutional Neural Network features visualization</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Convolutional-Neural-Network-Interpretability-with-General-Pattern-Theory"><span class="toc-number">40.</span> <span class="toc-text"># Convolutional Neural Network Interpretability with General Pattern Theory</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Mitigating-belief-projection-in-explainable-artificial-intelligence-via-Bayesian-Teaching"><span class="toc-number">41.</span> <span class="toc-text"># Mitigating belief projection in explainable artificial intelligence via Bayesian Teaching</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Neural-Network-Attribution-Methods-for-Problems-in-Geoscience-A-Novel-Synthetic-Benchmark-Dataset"><span class="toc-number">42.</span> <span class="toc-text"># Neural Network Attribution Methods for Problems in Geoscience: A Novel Synthetic Benchmark Dataset</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Local-Explanations-via-Necessity-and-Sufficiency-Unifying-Theory-and-Practice"><span class="toc-number">43.</span> <span class="toc-text"># Local Explanations via Necessity and Sufficiency: Unifying Theory and Practice</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Triplot-model-agnostic-measures-and-visualisations-for-variable-importance-in-predictive-models-that-take-into-account-the-hierarchical-correlation-structure"><span class="toc-number">44.</span> <span class="toc-text"># Triplot: model agnostic measures and visualisations for variable importance in predictive models that take into account the hierarchical correlation structure</span></a></li></ol>
        
    </div>
</aside>

</header>

        <div id="content" class="content">
            <div id="wrapper" class="wrapper" style="max-width: 800px">
                <article class="article" itemscope itemprop="blogPost">
    
    <header class="article-header">
        
        <h1 itemprop="name">
            XAI with code
        </h1>
        
        <div class="article-meta clearfix">
            <a class="article-date" href="https://adam-hohai.github.io/cenhelm.github.io/2022/05/23/XAI-with-code/index.html">
    
    <i class="icon-calendar vm"></i>
    
    <time class="vm" datetime="2022-05-23T10:55:00.000Z" itemprop="datePublished">2022-05-23</time>
</a>

            
<div class="article-tag-list">
    <i class="icon-tag vm"></i>
    <a class="article-tag-link" href="/cenhelm.github.io/tags/%E5%8F%AF%E8%A7%A3%E9%87%8A/" rel="tag">可解释</a>
</div>


        </div>
    </header>
    
    <section class="article-body markdown-body">
        
        <p>Explainable artificial intelligence with code 88篇论文汇总</p>
<span id="more"></span>
<h2 id="Axiomatic-Attribution-for-Deep-Networks"><a href="#Axiomatic-Attribution-for-Deep-Networks" class="headerlink" title="Axiomatic Attribution for Deep Networks"></a>Axiomatic Attribution for Deep Networks</h2><p>2017 2557<br>integrated gradients，将深度网络的预测归因于其输入。</p>
<h2 id="GNNExplainer-Generating-Explanations-for-Graph-Neural-Networks"><a href="#GNNExplainer-Generating-Explanations-for-Graph-Neural-Networks" class="headerlink" title="GNNExplainer: Generating Explanations for Graph Neural Networks"></a>GNNExplainer: Generating Explanations for Graph Neural Networks</h2><p>2019 316<br>为图神经网络生成解释，利用图NN的递归邻域聚合方案去辨别重要的图路径，突出显示沿着路径的边缘传递的相关节点特征信息。</p>
<h2 id="Proposed-Guidelines-for-the-Responsible-Use-of-Explainable-Machine-Learning"><a href="#Proposed-Guidelines-for-the-Responsible-Use-of-Explainable-Machine-Learning" class="headerlink" title="Proposed Guidelines for the Responsible Use of Explainable Machine Learning"></a>Proposed Guidelines for the Responsible Use of Explainable Machine Learning</h2><p>2019 14<br>综述类</p>
<h2 id="Software-for-Dataset-wide-XAI-From-Local-Explanations-to-Global-Insights-with-Zennit-CoRelAy-and-ViRelAy"><a href="#Software-for-Dataset-wide-XAI-From-Local-Explanations-to-Global-Insights-with-Zennit-CoRelAy-and-ViRelAy" class="headerlink" title="Software for Dataset-wide XAI: From Local Explanations to Global Insights with Zennit, CoRelAy, and ViRelAy"></a>Software for Dataset-wide XAI: From Local Explanations to Global Insights with Zennit, CoRelAy, and ViRelAy</h2><p>2021 11<br>Layer-wise Relevance Propagation (LRP)事后归因解释，产生局部解释，对单样本的额所有输入特征归因得分<br>以及相关的三个框架：Zennit，CoRelAy，CoRelAy</p>
<h2 id="SoPa-Bridging-CNNs-RNNs-and-Weighted-Finite-State-Machines"><a href="#SoPa-Bridging-CNNs-RNNs-and-Weighted-Finite-State-Machines" class="headerlink" title="SoPa: Bridging CNNs, RNNs, and Weighted Finite-State Machines"></a>SoPa: Bridging CNNs, RNNs, and Weighted Finite-State Machines</h2><p>2018 10<br>sopa模型将神经表示学习与加权有限状态自动机（WFSAs）结合，来学习传统表面模式的软版本。</p>
<h2 id="DO-NOT-TRUST-ADDITIVE-EXPLANATIONS"><a href="#DO-NOT-TRUST-ADDITIVE-EXPLANATIONS" class="headerlink" title="DO NOT TRUST ADDITIVE EXPLANATIONS"></a>DO NOT TRUST ADDITIVE EXPLANATIONS</h2><p>2019 17<br>像shap lime BD等这些加性解释不一定忠实，提出iBD方法捕获局部交互，并使用瀑布图可视化生成的非加性解释</p>
<h2 id="On-the-Explanation-of-Machine-Learning-Predictions-in-Clinical-Gait-Analysis"><a href="#On-the-Explanation-of-Machine-Learning-Predictions-in-Clinical-Gait-Analysis" class="headerlink" title="On the Explanation of Machine Learning Predictions in Clinical Gait Analysis"></a>On the Explanation of Machine Learning Predictions in Clinical Gait Analysis</h2><p>2020 5<br>LRP分层关联传播</p>
<h2 id="EUCA-the-End-User-Centered-Explainable-AI-Framework"><a href="#EUCA-the-End-User-Centered-Explainable-AI-Framework" class="headerlink" title="EUCA: the End-User-Centered Explainable AI Framework"></a>EUCA: the End-User-Centered Explainable AI Framework</h2><p>2021 0<br>框架</p>
<h2 id="Towards-Rigorous-Interpretations-a-Formalisation-of-Feature-Attribution"><a href="#Towards-Rigorous-Interpretations-a-Formalisation-of-Feature-Attribution" class="headerlink" title="Towards Rigorous Interpretations: a Formalisation of Feature Attribution"></a>Towards Rigorous Interpretations: a Formalisation of Feature Attribution</h2><p>2021 1<br>特征归因通常松散地表现为选择相关特征的子集作为预测的基本原理的过程。基于松弛函数依赖的概念的特征选择/归因形式化</p>
<h2 id="Entropy-based-Logic-Explanations-of-Neural-Networks基于熵的神经网络逻辑解释"><a href="#Entropy-based-Logic-Explanations-of-Neural-Networks基于熵的神经网络逻辑解释" class="headerlink" title="Entropy-based Logic Explanations of Neural Networks基于熵的神经网络逻辑解释"></a>Entropy-based Logic Explanations of Neural Networks基于熵的神经网络逻辑解释</h2><p>2021 6<br>新的端到端可微分的方法，使其能够利用一阶逻辑的形式主义从神经网络中提取逻辑解释。该方法依赖于一个基于熵的准则，它可以自动识别最相关的概念。</p>
<h2 id="Quantitative-Evaluation-of-Explainable-Graph-Neural-Networks-for-Molecular-Property-Prediction分子性质预测中可解释图神经网络的定量评价"><a href="#Quantitative-Evaluation-of-Explainable-Graph-Neural-Networks-for-Molecular-Property-Prediction分子性质预测中可解释图神经网络的定量评价" class="headerlink" title="Quantitative Evaluation of Explainable Graph Neural Networks for Molecular Property Prediction分子性质预测中可解释图神经网络的定量评价"></a>Quantitative Evaluation of Explainable Graph Neural Networks for Molecular Property Prediction分子性质预测中可解释图神经网络的定量评价</h2><p>2021 0<br>GNN</p>
<h2 id="Explaining-deep-learning-models-for-spoofing-and-deepfake-detection-with-SHapley-Additive-exPlanations"><a href="#Explaining-deep-learning-models-for-spoofing-and-deepfake-detection-with-SHapley-Additive-exPlanations" class="headerlink" title="Explaining deep learning models for spoofing and deepfake detection with SHapley Additive exPlanations"></a>Explaining deep learning models for spoofing and deepfake detection with SHapley Additive exPlanations</h2><p>2022 1<br>shap 欺骗和伪造检测</p>
<h2 id="GAM-e-changer-or-not-An-evaluation-of-interpretable-machine-learning-models-based-on-additive-model-constraints"><a href="#GAM-e-changer-or-not-An-evaluation-of-interpretable-machine-learning-models-based-on-additive-model-constraints" class="headerlink" title="# GAM(e) changer or not? An evaluation of interpretable machine learning models based on additive model constraints"></a># GAM(e) changer or not? An evaluation of interpretable machine learning models based on additive model constraints</h2><p>2022 0<br>广义加性模型</p>
<h2 id="Visual-Interpretability-for-Deep-Learning-a-Survey"><a href="#Visual-Interpretability-for-Deep-Learning-a-Survey" class="headerlink" title="# Visual Interpretability for Deep Learning: a Survey"></a># Visual Interpretability for Deep Learning: a Survey</h2><p>2018 589<br>综述 CNN的解释</p>
<h2 id="Meaningful-Data-Sampling-for-a-Faithful-Local-Explanation-Method"><a href="#Meaningful-Data-Sampling-for-a-Faithful-Local-Explanation-Method" class="headerlink" title="# Meaningful Data Sampling for a Faithful Local Explanation Method"></a># Meaningful Data Sampling for a Faithful Local Explanation Method</h2><p>2019 3<br>对于lime提出一个新的采样方法以获取扰动样本</p>
<h2 id="Explainable-Artificial-Intelligence-XAI-Concepts-Taxonomies-Opportunities-and-Challenges-toward-Responsible-AI"><a href="#Explainable-Artificial-Intelligence-XAI-Concepts-Taxonomies-Opportunities-and-Challenges-toward-Responsible-AI" class="headerlink" title="# Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI"></a># Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI</h2><p>2020 2196<br>综述</p>
<h2 id="Towards-Best-Practice-in-Explaining-Neural-Network-Decisions-with-LRP"><a href="#Towards-Best-Practice-in-Explaining-Neural-Network-Decisions-with-LRP" class="headerlink" title="# Towards Best Practice in Explaining Neural Network Decisions with LRP"></a># Towards Best Practice in Explaining Neural Network Decisions with LRP</h2><p>2020 65<br>LRP</p>
<h2 id="bLIMEy-Surrogate-Prediction-Explanations-Beyond-LIME"><a href="#bLIMEy-Surrogate-Prediction-Explanations-Beyond-LIME" class="headerlink" title="# bLIMEy: Surrogate Prediction Explanations Beyond LIME"></a># bLIMEy: Surrogate Prediction Explanations Beyond LIME</h2><p>2019 16<br>基于lime的一个框架</p>
<h2 id="Rule-Extraction-in-Unsupervised-Anomaly-Detection-for-Model-Explainability-Application-to-OneClass-SVM"><a href="#Rule-Extraction-in-Unsupervised-Anomaly-Detection-for-Model-Explainability-Application-to-OneClass-SVM" class="headerlink" title="# Rule Extraction in Unsupervised Anomaly Detection for Model Explainability: Application to OneClass SVM"></a># Rule Extraction in Unsupervised Anomaly Detection for Model Explainability: Application to OneClass SVM</h2><p>2022 8<br>异常检测<br>OneClass SVM模型<br>规则提取</p>
<h2 id="What-Would-You-Ask-the-Machine-Learning-Model-Identification-of-User-Needs-for-Model-Explanations-Based-on-Human-Model-Conversations"><a href="#What-Would-You-Ask-the-Machine-Learning-Model-Identification-of-User-Needs-for-Model-Explanations-Based-on-Human-Model-Conversations" class="headerlink" title="# What Would You Ask the Machine Learning Model? Identification of User Needs for Model Explanations Based on Human-Model Conversations"></a># What Would You Ask the Machine Learning Model? Identification of User Needs for Model Explanations Based on Human-Model Conversations</h2><p>2020 4<br>人机对话系统 收集人类操作员需求</p>
<h2 id="Learning-to-Structure-an-Image-with-Few-Colors"><a href="#Learning-to-Structure-an-Image-with-Few-Colors" class="headerlink" title="# Learning to Structure an Image with Few Colors"></a># Learning to Structure an Image with Few Colors</h2><p>2020 3<br>图像 ColorCNN</p>
<h2 id="The-Grammar-of-Interactive-Explanatory-Model-Analysis"><a href="#The-Grammar-of-Interactive-Explanatory-Model-Analysis" class="headerlink" title="# The Grammar of Interactive Explanatory Model Analysis"></a># The Grammar of Interactive Explanatory Model Analysis</h2><p>2022 13<br>模型的交互式顺序分析 框架 解释模型分析EMA</p>
<h2 id="Applying-Genetic-Programming-to-Improve-Interpretability-in-Machine-Learning-Models"><a href="#Applying-Genetic-Programming-to-Improve-Interpretability-in-Machine-Learning-Models" class="headerlink" title="# Applying Genetic Programming to Improve Interpretability in Machine Learning Models"></a># Applying Genetic Programming to Improve Interpretability in Machine Learning Models</h2><p>2020 8<br>使用遗传算法生成兴趣点的邻域，拟合一个局部解释模型</p>
<h2 id="Explanations-of-Black-Box-Model-Predictions-by-Contextual-Importance-and-Utility"><a href="#Explanations-of-Black-Box-Model-Predictions-by-Contextual-Importance-and-Utility" class="headerlink" title="# Explanations of Black-Box Model Predictions by Contextual Importance and Utility"></a># Explanations of Black-Box Model Predictions by Contextual Importance and Utility</h2><p>2019 22<br>上下文重要性CI 上下文效用CU</p>
<h2 id="Explaining-Local-Global-And-Higher-Order-Interactions-In-Deep-Learning"><a href="#Explaining-Local-Global-And-Higher-Order-Interactions-In-Deep-Learning" class="headerlink" title="# Explaining Local, Global, And Higher-Order Interactions In Deep Learning"></a># Explaining Local, Global, And Higher-Order Interactions In Deep Learning</h2><p>2021 1<br>交叉导数 视觉 Grad-CAM</p>
<h2 id="Embedded-Encoder-Decoder-in-Convolutional-Networks-Towards-Explainable-AI"><a href="#Embedded-Encoder-Decoder-in-Convolutional-Networks-Towards-Explainable-AI" class="headerlink" title="# Embedded Encoder-Decoder in Convolutional Networks Towards Explainable AI"></a># Embedded Encoder-Decoder in Convolutional Networks Towards Explainable AI</h2><p>2020 2<br>CNN 编码器解码器 热图</p>
<h2 id="EXPLAN-Explaining-Black-box-Classifiers-using-Adaptive-Neighborhood-Generation"><a href="#EXPLAN-Explaining-Black-box-Classifiers-using-Adaptive-Neighborhood-Generation" class="headerlink" title="# EXPLAN: Explaining Black-box Classifiers using Adaptive Neighborhood Generation"></a># EXPLAN: Explaining Black-box Classifiers using Adaptive Neighborhood Generation</h2><p>2020 5<br>邻域 逻辑规则</p>
<h2 id="XNAP-Making-LSTM-based-Next-Activity-Predictions-Explainable-by-Using-LRP"><a href="#XNAP-Making-LSTM-based-Next-Activity-Predictions-Explainable-by-Using-LRP" class="headerlink" title="# XNAP: Making LSTM-based Next Activity Predictions Explainable by Using LRP"></a># XNAP: Making LSTM-based Next Activity Predictions Explainable by Using LRP</h2><p>2020 10<br>LSTM LRP</p>
<h2 id="Quantifying-Explainability-of-Saliency-Methods-in-Deep-Neural-Networks-with-a-Synthetic-Dataset"><a href="#Quantifying-Explainability-of-Saliency-Methods-in-Deep-Neural-Networks-with-a-Synthetic-Dataset" class="headerlink" title="# Quantifying Explainability of Saliency Methods in Deep Neural Networks with a Synthetic Dataset"></a># Quantifying Explainability of Saliency Methods in Deep Neural Networks with a Synthetic Dataset</h2><p>2020 0<br>DNN 热图</p>
<h2 id="TripleTree-A-Versatile-Interpretable-Representation-of-Black-Box-Agents-and-their-Environments"><a href="#TripleTree-A-Versatile-Interpretable-Representation-of-Black-Box-Agents-and-their-Environments" class="headerlink" title="# TripleTree: A Versatile Interpretable Representation of Black Box Agents and their Environments"></a># TripleTree: A Versatile Interpretable Representation of Black Box Agents and their Environments</h2><p>2020 5<br>强化学习 cart决策树</p>
<h2 id="SCOUTER-Slot-Attention-based-Classifier-for-Explainable-Image-Recognition"><a href="#SCOUTER-Slot-Attention-based-Classifier-for-Explainable-Image-Recognition" class="headerlink" title="# SCOUTER: Slot Attention-based Classifier for Explainable Image Recognition"></a># SCOUTER: Slot Attention-based Classifier for Explainable Image Recognition</h2><p>2021 7<br>图像 attention</p>
<h2 id="Impact-of-lung-segmentation-on-the-diagnosis-and-explanation-of-COVID-19-in-chest-X-ray-images"><a href="#Impact-of-lung-segmentation-on-the-diagnosis-and-explanation-of-COVID-19-in-chest-X-ray-images" class="headerlink" title="# Impact of lung segmentation on the diagnosis and explanation of COVID-19 in chest X-ray images"></a># Impact of lung segmentation on the diagnosis and explanation of COVID-19 in chest X-ray images</h2><p>2021 33<br>CNN</p>
<h2 id="Landscape-of-R-packages-for-eXplainable-Artificial-Intelligence"><a href="#Landscape-of-R-packages-for-eXplainable-Artificial-Intelligence" class="headerlink" title="# Landscape of R packages for eXplainable Artificial Intelligence"></a># Landscape of R packages for eXplainable Artificial Intelligence</h2><p>2020 7<br>框架 R</p>
<h2 id="Causal-Shapley-Values-Exploiting-Causal-Knowledge-to-Explain-Individual-Predictions-of-Complex-Models"><a href="#Causal-Shapley-Values-Exploiting-Causal-Knowledge-to-Explain-Individual-Predictions-of-Complex-Models" class="headerlink" title="# Causal Shapley Values: Exploiting Causal Knowledge to Explain Individual Predictions of Complex Models"></a># Causal Shapley Values: Exploiting Causal Knowledge to Explain Individual Predictions of Complex Models</h2><p>2020 33<br>因果shapley值</p>
<h2 id="Why-model-why-Assessing-the-strengths-and-limitations-of-LIME"><a href="#Why-model-why-Assessing-the-strengths-and-limitations-of-LIME" class="headerlink" title="# Why model why? Assessing the strengths and limitations of LIME"></a># Why model why? Assessing the strengths and limitations of LIME</h2><p>2020 15<br>LIME 优缺点</p>
<h2 id="Driving-Behavior-Explanation-with-Multi-level-Fusion"><a href="#Driving-Behavior-Explanation-with-Multi-level-Fusion" class="headerlink" title="# Driving Behavior Explanation with Multi-level Fusion"></a># Driving Behavior Explanation with Multi-level Fusion</h2><p>2022 6<br>自动驾驶 解释轨迹预测模型</p>
<h2 id="Checklist-for-responsible-deep-learning-modeling-of-medical-images-based-on-COVID-19-detection-studies"><a href="#Checklist-for-responsible-deep-learning-modeling-of-medical-images-based-on-COVID-19-detection-studies" class="headerlink" title="# Checklist for responsible deep learning modeling of medical images based on COVID-19 detection studies"></a># Checklist for responsible deep learning modeling of medical images based on COVID-19 detection studies</h2><p>2021 9<br>图像 医疗</p>
<h2 id="XAI-P-T-A-Brief-Review-of-Explainable-Artificial-Intelligence-from-Practice-to-Theory"><a href="#XAI-P-T-A-Brief-Review-of-Explainable-Artificial-Intelligence-from-Practice-to-Theory" class="headerlink" title="# XAI-P-T: A Brief Review of Explainable Artificial Intelligence from Practice to Theory"></a># XAI-P-T: A Brief Review of Explainable Artificial Intelligence from Practice to Theory</h2><p>2022 2<br>综述</p>
<h2 id="TorchPRISM-Principal-Image-Sections-Mapping-a-novel-method-for-Convolutional-Neural-Network-features-visualization"><a href="#TorchPRISM-Principal-Image-Sections-Mapping-a-novel-method-for-Convolutional-Neural-Network-features-visualization" class="headerlink" title="# TorchPRISM: Principal Image Sections Mapping, a novel method for Convolutional Neural Network features visualization"></a># TorchPRISM: Principal Image Sections Mapping, a novel method for Convolutional Neural Network features visualization</h2><p>2021 0<br>CNN 可视化</p>
<h2 id="Convolutional-Neural-Network-Interpretability-with-General-Pattern-Theory"><a href="#Convolutional-Neural-Network-Interpretability-with-General-Pattern-Theory" class="headerlink" title="# Convolutional Neural Network Interpretability with General Pattern Theory"></a># Convolutional Neural Network Interpretability with General Pattern Theory</h2><p>2021 3<br>CNN 热图</p>
<h2 id="Mitigating-belief-projection-in-explainable-artificial-intelligence-via-Bayesian-Teaching"><a href="#Mitigating-belief-projection-in-explainable-artificial-intelligence-via-Bayesian-Teaching" class="headerlink" title="# Mitigating belief projection in explainable artificial intelligence via Bayesian Teaching"></a># Mitigating belief projection in explainable artificial intelligence via Bayesian Teaching</h2><p>2021 17<br>贝叶斯教学</p>
<h2 id="Neural-Network-Attribution-Methods-for-Problems-in-Geoscience-A-Novel-Synthetic-Benchmark-Dataset"><a href="#Neural-Network-Attribution-Methods-for-Problems-in-Geoscience-A-Novel-Synthetic-Benchmark-Dataset" class="headerlink" title="# Neural Network Attribution Methods for Problems in Geoscience: A Novel Synthetic Benchmark Dataset"></a># Neural Network Attribution Methods for Problems in Geoscience: A Novel Synthetic Benchmark Dataset</h2><p>2021 14<br>NN归因方法 热图</p>
<h2 id="Local-Explanations-via-Necessity-and-Sufficiency-Unifying-Theory-and-Practice"><a href="#Local-Explanations-via-Necessity-and-Sufficiency-Unifying-Theory-and-Practice" class="headerlink" title="# Local Explanations via Necessity and Sufficiency: Unifying Theory and Practice"></a># Local Explanations via Necessity and Sufficiency: Unifying Theory and Practice</h2><p>2021 6<br>必要性 充分性</p>
<h2 id="Triplot-model-agnostic-measures-and-visualisations-for-variable-importance-in-predictive-models-that-take-into-account-the-hierarchical-correlation-structure"><a href="#Triplot-model-agnostic-measures-and-visualisations-for-variable-importance-in-predictive-models-that-take-into-account-the-hierarchical-correlation-structure" class="headerlink" title="# Triplot: model agnostic measures and visualisations for variable importance in predictive models that take into account the hierarchical correlation structure"></a># Triplot: model agnostic measures and visualisations for variable importance in predictive models that take into account the hierarchical correlation structure</h2><p>2021 4<br>特征相关性 局部解释</p>

        
    </section>
</article>



<a id="pagenext" href="/cenhelm.github.io/2022/04/24/%E5%8F%AF%E8%A7%A3%E9%87%8A%E6%96%B9%E6%B3%95%E7%9A%84%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/" class="article-next" title="可解释方法的评价指标"><i class="icon-arrow-right"></i></a>





            </div>
        </div>
        <footer class="footer">
    Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, Theme by <a href="https://github.com/sanonz/hexo-theme-concise" target="_blank">Concise</a>

    
    <script>
        var _hmt = _hmt || [];
        (function () {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?d8d752057b9f8a189a988435e7d5c309";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();
    </script>
    
</footer>

    </main>

    <script type="text/javascript" src="https://unpkg.com/jquery@1.9.1/jquery.min.js"></script>
    <script type="text/javascript">
    $(function() {
        var nodes = {
            nav: $('#nav'),
            aside: $('#aside'),
            asideInner: $('#aside-inner'),
            navInner: $('#nav-inner')
        };

        var doing = false;
        nodes.asideInner.on('webkitAnimationEnd mozAnimationEnd oAnimationEnd oanimationend animationend', function() {
            if (nodes.aside.hasClass('mobile-open')) {
                nodes.aside.removeClass('mobile-open');
            } else {
                nodes.aside.removeClass('mobile-close panel-show');
            }
            doing = false;
        });
        $('#open-panel, #aside-mask').on('click', function() {
            if (doing) {
                return;
            }

            if (nodes.aside.hasClass('panel-show')) {
                nodes.aside.addClass('mobile-close');
            } else {
                nodes.aside.addClass('mobile-open panel-show');
            }
        });
        $('#open-menus').on('click', function() {
            nodes.navInner.slideToggle('normal', slideDone);
        });

        if (window.innerWidth <= 960) {
            setTimeout(function() {
                nodes.navInner.slideUp('normal', slideDone);
            }, 3000);
        }

        function slideDone() {
            if (nodes.navInner.css('display') !== 'none') {
                nodes.navInner.css('display', '');
            }
        }

        $(window).on('resize', function() {
            if ($(this).width() > 960) {
                nodes.navInner.css('display', '');
            }
        });
    });
    </script>
    
        
<script src="/cenhelm.github.io/js/scrollspy.min.js"></script>

        <script type="text/javascript">
        $(document.body).scrollspy({target: '#aside-inner'});

        $(window).on('resize', function() {
            var hw = $('#header').width();
            var ww = $('#wrapper').width();
            var space = ($(this).width() - hw - ww) / 2 / 2;

            var pageprev = $('#pageprev');
            var pagenext = $('#pagenext');
            var avg = (pageprev.width() + pagenext.width()) / 2

            if(space > avg) {
                var len = space - avg / 2;
                var styles = {position: 'fixed', top: '50%', marginTop: - (pageprev.width() + pagenext.width()) / 4}
                pageprev.css($.extend({left: hw + len}, styles));
                pagenext.css($.extend({right: len}, styles));
            } else {
                pageprev.removeAttr('style');
                pagenext.removeAttr('style');
            }
        }).trigger('resize');
        </script>
    

<script src="/cenhelm.github.io/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/cenhelm.github.io/live2dw/assets/shizuku.model.json"},"display":{"position":"left","width":150,"height":300,"hOffset":150},"mobile":{"show":false},"log":false});</script></body>
</html>
