<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8" />

    

    

    <title>NN相关方法和框架 | Cenehlm&#39;s blogs</title>
    <meta name="author" content="Cenehlm" />
    <meta name="keywords" content="" />
    <meta name="description" content="NN相关概念，及框架，主要介绍keras相关概念batch这个概念与Keras无关，老实讲不应该出现在这里的，但是因为它频繁出现，而且不了解这个技术的话看函数说明会很头痛，这里还是简单说一下。深度学习的优化算法，说白了就是梯度下降。每次的参数更新有两种方式。第一种，遍历全部数据集算一次损失函数，然后算函数对各个参数的梯度，更新梯度。这种方法每更新一次参数都要把数据集里的所有样本都看一遍，计算量开销大，计算速度慢，不支持在线学习，这称为Batch gradient descent，批梯度下降。另一" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
    <meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=no" />

    
    <link rel="alternate" href="/cenhelm.github.io/atom.xml" title="Cenehlm&#39;s blogs" type="application/atom+xml">
    
    
    <link rel="icon" href="/cenhelm.github.io/images/favicon.ico">
    

    <style type="text/css">
    @font-face {
        font-family: 'icomoon';
        src: url("/cenhelm.github.io/fonts/icomoon.eot?q628ml");
        src: url("/cenhelm.github.io/fonts/icomoon.eot?q628ml#iefix") format('embedded-opentype'),
             url("/cenhelm.github.io/fonts/icomoon.ttf?q628ml") format('truetype'),
             url("/cenhelm.github.io/fonts/icomoon.woff?q628ml") format('woff'),
             url("/cenhelm.github.io/fonts/icomoon.svg?q628ml#icomoon") format('svg');
        font-weight: normal;
        font-style: normal;
    }
    </style>
    
<link rel="stylesheet" href="/cenhelm.github.io/css/style.css">


    <!--[if lt IE 9]><style type="text/css">.nav-inner {top:0;}.author-meta {position:static;top:0;}.search-form {height:36px;}</style><script type="text/javascript" src="https://unpkg.com/html5shiv@3.7.3/dist/html5shiv.min.js"></script><![endif]-->
<meta name="generator" content="Hexo 5.4.0"></head>
<body>

    <main class="app">
        <header id="header" class="header clearfix">
    <div id="nav" class="nav">
    <div class="nav-mobile">
        <button id="open-panel" class="open-panel nav-mobile-item"><i class="icon-documents"></i></button>
        <h1 class="nav-mobile-title nav-mobile-item">Cenehlm&#39;s blogs</h1>
        <button id="open-menus" class="open-panel nav-mobile-item"><i class="icon-library"></i></button>
    </div>

    <nav id="nav-inner" class="nav-inner">
        
            <a class="nav-item" href="/cenhelm.github.io/">
                <span class="nav-text">首页</span>
            </a>
        
            <a class="nav-item" href="/cenhelm.github.io/categories/%E6%97%A5%E5%B8%B8">
                <span class="nav-text">日常</span>
            </a>
        
            <a class="nav-item" href="/cenhelm.github.io/categories/%E6%AF%94%E8%B5%9B">
                <span class="nav-text">比赛</span>
            </a>
        
            <a class="nav-item" href="/cenhelm.github.io/archives">
                <span class="nav-text">归档</span>
            </a>
        
            <a class="nav-item" href="/cenhelm.github.io/about">
                <span class="nav-text">关于</span>
            </a>
        
    </nav>
</div>

    <aside id="aside" class="aside">
    <div id="aside-mask" class="aside-mask"></div>
    <div id="aside-inner" class="aside-inner">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit"><i class="icon-search-stroke"></i></button><input type="hidden" name="sitesearch" value="https://adam-hohai.github.io/cenhelm.github.io"></form>

        
        
        
        

        
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5"><span class="toc-number">1.</span> <span class="toc-text">相关概念</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#batch"><span class="toc-number">1.1.</span> <span class="toc-text">batch</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F"><span class="toc-number">1.2.</span> <span class="toc-text">张量</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#input-shape"><span class="toc-number">1.3.</span> <span class="toc-text">input_shape</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BC%96%E8%AF%91"><span class="toc-number">2.</span> <span class="toc-text">编译</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B8%B8%E7%94%A8%E5%B1%82"><span class="toc-number">3.</span> <span class="toc-text">常用层</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Dense%E5%B1%82"><span class="toc-number">3.1.</span> <span class="toc-text">Dense层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Activation%E5%B1%82"><span class="toc-number">3.2.</span> <span class="toc-text">Activation层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Dropout%E5%B1%82"><span class="toc-number">3.3.</span> <span class="toc-text">Dropout层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Flatten%E5%B1%82"><span class="toc-number">3.4.</span> <span class="toc-text">Flatten层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Reshape%E5%B1%82"><span class="toc-number">3.5.</span> <span class="toc-text">Reshape层</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BE%AA%E7%8E%AF%E5%B1%82Recurrent"><span class="toc-number">4.</span> <span class="toc-text">循环层Recurrent</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B1%8F%E8%94%BD%E8%BE%93%E5%85%A5%E6%95%B0%E6%8D%AE%EF%BC%88Masking%EF%BC%89"><span class="toc-number">4.1.</span> <span class="toc-text">屏蔽输入数据（Masking）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#stateful"><span class="toc-number">4.2.</span> <span class="toc-text">stateful</span></a></li></ol></li></ol>
        
    </div>
</aside>

</header>

        <div id="content" class="content">
            <div id="wrapper" class="wrapper" style="max-width: 800px">
                <article class="article" itemscope itemprop="blogPost">
    
    <header class="article-header">
        
        <h1 itemprop="name">
            NN相关方法和框架
        </h1>
        
        <div class="article-meta clearfix">
            <a class="article-date" href="https://adam-hohai.github.io/cenhelm.github.io/2022/03/18/NN%E7%9B%B8%E5%85%B3%E6%96%B9%E6%B3%95%E5%92%8C%E6%A1%86%E6%9E%B6/index.html">
    
    <i class="icon-calendar vm"></i>
    
    <time class="vm" datetime="2022-03-18T09:30:46.000Z" itemprop="datePublished">2022-03-18</time>
</a>

            
<div class="article-tag-list">
    <i class="icon-tag vm"></i>
    <a class="article-tag-link" href="/cenhelm.github.io/tags/DL/" rel="tag">DL</a>
</div>


        </div>
    </header>
    
    <section class="article-body markdown-body">
        
        <p>NN相关概念，及框架，主要介绍keras</p>
<span id="more"></span>
<h2 id="相关概念"><a href="#相关概念" class="headerlink" title="相关概念"></a>相关概念</h2><h3 id="batch"><a href="#batch" class="headerlink" title="batch"></a>batch</h3><p>这个概念与Keras无关，老实讲不应该出现在这里的，但是因为它频繁出现，而且不了解这个技术的话看函数说明会很头痛，这里还是简单说一下。</p>
<p>深度学习的优化算法，说白了就是梯度下降。每次的参数更新有两种方式。</p>
<p>第一种，遍历全部数据集算一次损失函数，然后算函数对各个参数的梯度，更新梯度。这种方法每更新一次参数都要把数据集里的所有样本都看一遍，计算量开销大，计算速度慢，不支持在线学习，这称为Batch gradient descent，批梯度下降。</p>
<p>另一种，每看一个数据就算一下损失函数，然后求梯度更新参数，这个称为随机梯度下降，stochastic gradient descent。这个方法速度比较快，但是收敛性能不太好，可能在最优点附近晃来晃去，hit不到最优点。两次参数的更新也有可能互相抵消掉，造成目标函数震荡的比较剧烈。</p>
<p>为了克服两种方法的缺点，现在一般采用的是一种折中手段，mini-batch gradient decent，小批的梯度下降，这种方法把数据分为若干个批，按批来更新参数，这样，一个批中的一组数据共同决定了本次梯度的方向，下降起来就不容易跑偏，减少了随机性。另一方面因为批的样本数与整个数据集相比小了很多，计算量也不是很大。</p>
<p>基本上现在的梯度下降都是基于mini-batch的，所以Keras的模块中经常会出现batch_size，就是指这个。</p>
<p>顺便说一句，Keras中用的优化器SGD是stochastic gradient descent的缩写，但不代表是一个样本就更新一回，还是基于mini-batch的。</p>
<h3 id="张量"><a href="#张量" class="headerlink" title="张量"></a>张量</h3><p>去除量纲的数据类型<br><strong><img src="/cenhelm.github.io/2022/03/18/NN%E7%9B%B8%E5%85%B3%E6%96%B9%E6%B3%95%E5%92%8C%E6%A1%86%E6%9E%B6/1.png" class></strong></p>
<h3 id="input-shape"><a href="#input-shape" class="headerlink" title="input_shape"></a>input_shape</h3><p><strong><img src="/cenhelm.github.io/2022/03/18/NN%E7%9B%B8%E5%85%B3%E6%96%B9%E6%B3%95%E5%92%8C%E6%A1%86%E6%9E%B6/3.png" class></strong><br>不考虑batch大小，为单个样本的shape</p>
<h2 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h2><p><strong><img src="/cenhelm.github.io/2022/03/18/NN%E7%9B%B8%E5%85%B3%E6%96%B9%E6%B3%95%E5%92%8C%E6%A1%86%E6%9E%B6/2.png" class></strong></p>
<h2 id="常用层"><a href="#常用层" class="headerlink" title="常用层"></a>常用层</h2><h3 id="Dense层"><a href="#Dense层" class="headerlink" title="Dense层"></a>Dense层</h3><p>Dense就是常用的全连接层，所实现的运算是output = activation(dot(input, kernel)+bias)，输入及其对应权值的点积加上一个偏置向量，最后套一个激活函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># as first layer in a sequential model:</span></span><br><span class="line"><span class="comment"># as first layer in a sequential model:</span></span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(Dense(<span class="number">32</span>, input_shape=(<span class="number">16</span>,)))</span><br><span class="line"><span class="comment"># now the model will take as input arrays of shape (*, 16)</span></span><br><span class="line"><span class="comment"># and output arrays of shape (*, 32)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># after the first layer, you don&#x27;t need to specify</span></span><br><span class="line"><span class="comment"># the size of the input anymore:</span></span><br><span class="line">model.add(Dense(<span class="number">32</span>))</span><br></pre></td></tr></table></figure>
<h3 id="Activation层"><a href="#Activation层" class="headerlink" title="Activation层"></a>Activation层</h3><p>当使用激活层作为第一层时，要指定input_shape</p>
<h3 id="Dropout层"><a href="#Dropout层" class="headerlink" title="Dropout层"></a>Dropout层</h3><p>为输入数据施加Dropout。Dropout将在训练过程中每次更新参数时按一定概率（rate）随机断开输入神经元，Dropout层用于防止过拟合。<br><strong><img src="/cenhelm.github.io/2022/03/18/NN%E7%9B%B8%E5%85%B3%E6%96%B9%E6%B3%95%E5%92%8C%E6%A1%86%E6%9E%B6/4.png" class></strong></p>
<h3 id="Flatten层"><a href="#Flatten层" class="headerlink" title="Flatten层"></a>Flatten层</h3><p>Flatten层用来将输入“压平”，即把多维的输入一维化，常用在从卷积层到全连接层的过渡。Flatten不影响batch的大小。</p>
<h3 id="Reshape层"><a href="#Reshape层" class="headerlink" title="Reshape层"></a>Reshape层</h3><p>Reshape层用来将输入shape转换为特定的shape</p>
<h2 id="循环层Recurrent"><a href="#循环层Recurrent" class="headerlink" title="循环层Recurrent"></a>循环层Recurrent</h2><p>这是循环层的抽象类，请不要在模型中直接应用该层（因为它是抽象类，无法实例化任何对象）。请使用它的子类LSTM，GRU或SimpleRNN。<br><strong><img src="/cenhelm.github.io/2022/03/18/NN%E7%9B%B8%E5%85%B3%E6%96%B9%E6%B3%95%E5%92%8C%E6%A1%86%E6%9E%B6/5.png" class></strong><br>有多个循环层时需要加return_sequences=True</p>
<h3 id="屏蔽输入数据（Masking）"><a href="#屏蔽输入数据（Masking）" class="headerlink" title="屏蔽输入数据（Masking）"></a>屏蔽输入数据（Masking）</h3><p>循环层支持通过时间步变量对输入数据进行Masking，如果想将输入数据的一部分屏蔽掉，请使用Embedding层并将参数mask_zero设为True。</p>
<h3 id="stateful"><a href="#stateful" class="headerlink" title="stateful"></a>stateful</h3><p>每个batch计算出的状态都会被重用于初始化下一个batch的初始状态，状态RNN假设连续的两个batch之中，相同下表的元素就有一一映射关系。<br>要启用状态RNN，请在实例化层对象时指定参数stateful=True，并在Sequential模型使用固定大小的batch：通过在模型的第一层传入batch_size=(…)和input_shape来实现。在函数式模型中，对所有的输入都要指定相同的batch_size。</p>

        
    </section>
</article>



<a id="pagenext" href="/cenhelm.github.io/2022/03/18/leetcode-%E9%93%BE%E8%A1%A8/" class="article-next" title="leetcode-链表"><i class="icon-arrow-right"></i></a>


<a id="pageprev" href="/cenhelm.github.io/2022/03/18/LSTM%E7%BB%BC%E8%BF%B0/" class="article-prev" title="LSTM综述"><i class="icon-arrow-left"></i></a>




            </div>
        </div>
        <footer class="footer">
    Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>, Theme by <a href="https://github.com/sanonz/hexo-theme-concise" target="_blank">Concise</a>

    
    <script>
        var _hmt = _hmt || [];
        (function () {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?d8d752057b9f8a189a988435e7d5c309";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();
    </script>
    
</footer>

    </main>

    <script type="text/javascript" src="https://unpkg.com/jquery@1.9.1/jquery.min.js"></script>
    <script type="text/javascript">
    $(function() {
        var nodes = {
            nav: $('#nav'),
            aside: $('#aside'),
            asideInner: $('#aside-inner'),
            navInner: $('#nav-inner')
        };

        var doing = false;
        nodes.asideInner.on('webkitAnimationEnd mozAnimationEnd oAnimationEnd oanimationend animationend', function() {
            if (nodes.aside.hasClass('mobile-open')) {
                nodes.aside.removeClass('mobile-open');
            } else {
                nodes.aside.removeClass('mobile-close panel-show');
            }
            doing = false;
        });
        $('#open-panel, #aside-mask').on('click', function() {
            if (doing) {
                return;
            }

            if (nodes.aside.hasClass('panel-show')) {
                nodes.aside.addClass('mobile-close');
            } else {
                nodes.aside.addClass('mobile-open panel-show');
            }
        });
        $('#open-menus').on('click', function() {
            nodes.navInner.slideToggle('normal', slideDone);
        });

        if (window.innerWidth <= 960) {
            setTimeout(function() {
                nodes.navInner.slideUp('normal', slideDone);
            }, 3000);
        }

        function slideDone() {
            if (nodes.navInner.css('display') !== 'none') {
                nodes.navInner.css('display', '');
            }
        }

        $(window).on('resize', function() {
            if ($(this).width() > 960) {
                nodes.navInner.css('display', '');
            }
        });
    });
    </script>
    
        
<script src="/cenhelm.github.io/js/scrollspy.min.js"></script>

        <script type="text/javascript">
        $(document.body).scrollspy({target: '#aside-inner'});

        $(window).on('resize', function() {
            var hw = $('#header').width();
            var ww = $('#wrapper').width();
            var space = ($(this).width() - hw - ww) / 2 / 2;

            var pageprev = $('#pageprev');
            var pagenext = $('#pagenext');
            var avg = (pageprev.width() + pagenext.width()) / 2

            if(space > avg) {
                var len = space - avg / 2;
                var styles = {position: 'fixed', top: '50%', marginTop: - (pageprev.width() + pagenext.width()) / 4}
                pageprev.css($.extend({left: hw + len}, styles));
                pagenext.css($.extend({right: len}, styles));
            } else {
                pageprev.removeAttr('style');
                pagenext.removeAttr('style');
            }
        }).trigger('resize');
        </script>
    

<script src="/cenhelm.github.io/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/cenhelm.github.io/live2dw/assets/shizuku.model.json"},"display":{"position":"left","width":150,"height":300,"hOffset":150},"mobile":{"show":false},"log":false});</script></body>
</html>
